{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb3c5d8",
   "metadata": {},
   "source": [
    "# Demo 1: Complete Hessian Eigenvector-Based Loss Landscape Analysis\n",
    "\n",
    "This comprehensive notebook demonstrates the **complete end-to-end workflow** for computing Hessian eigenvectors from a trained ALIGNN model and generating 2D loss landscapes for materials property prediction analysis.\n",
    "\n",
    "## **What This Demo Covers**\n",
    "\n",
    "This notebook provides a **complete, self-contained tutorial** covering two major phases:\n",
    "\n",
    "### **Part 1: Hessian Eigenvector Computation**\n",
    "1. **Model and Data Loading** - Load demo model and demo dataset\n",
    "2. **Prediction and Error Analysis** - Make predictions and identify lowest-error samples (Hessian computation is resource intensive. We always choose a smaller subset such as lowest prediction error samples, random samples, input-feature based selection, etc., to approximate the full dataset.)  \n",
    "3. **Hessian Matrix Computation** - Compute maximum and minimum eigenvectors of the loss Hessian of the subset of samples\n",
    "4. **Eigenvector Processing** - Convert eigenvectors to model weight format\n",
    "5. **Saving** - Create and save eigenvectors\n",
    "\n",
    "### **Part 2: Loss Landscape Generation and Analysis**\n",
    "6. **2D Landscape Generation** - Use planar interpolation between the original model and the 2 (max&min) eigenvector models computed from the subset of dataset to generate loss landscapes for the full dataset.\n",
    "7. **Visualization** - Create informative plots of loss surface topology\n",
    "\n",
    "## **Scientific Context**\n",
    "\n",
    "**Loss landscapes** reveal the topology of the loss function around a trained model, providing insights into:\n",
    "- **Local minima structure** and escape paths  \n",
    "- **Sensitivity to parameter perturbations**\n",
    "- **Relationship between loss geometry and prediction quality**\n",
    "\n",
    "**Hessian eigenvectors** define the principal directions of curvature in the loss surface, representing:\n",
    "- **Maximum curvature direction** (steepest/sharpest changes)\n",
    "- **Minimum curvature direction** (flattest/most stable changes)\n",
    "\n",
    "## **Technical Details**\n",
    "\n",
    "### **Dataset**: \n",
    "- **Source**: JARVIS-DFT (Joint Automated Repository for Various Integrated Simulations)\n",
    "- **Property**: Formation energy (dHf) prediction\n",
    "- **Samples**: 50 random materials from the database for demo purposes\n",
    "- **Target Selection**: 20 lowest-error samples for eigenvector computation for demo purposes\n",
    "\n",
    "### **Model**: \n",
    "- **Architecture**: ALIGNN (Atomistic Line Graph Neural Network)\n",
    "- **Task**: Regression on formation energy values\n",
    "- **Parameters**: ~4 million\n",
    "- **Training**: Pre-trained on JARVIS-DFT formation energy data\n",
    "\n",
    "### **Computational Requirements**:\n",
    "- **Memory**: Hessian computation requires significant GPU memory\n",
    "- **Time**: Complete workflow takes 10 minutes on a RTX4070 GPU\n",
    "\n",
    "### **Key Algorithms**:\n",
    "- **Hessian-Vector Products**: Efficient computation without storing full Hessian matrix\n",
    "- **Power Iteration**: For computing dominant eigenvectors\n",
    "- **Planar Interpolation**: Linear combinations of three models (original + 2 eigenvectors)\n",
    "\n",
    "## **Output Files Generated**\n",
    "\n",
    "This notebook creates several important output files:\n",
    "\n",
    "### **Low Error Subset**:\n",
    "- `demo_JVDFT_dHf_dataset_lowest_20_error_samples_from_50` - lowest error samples\n",
    "\n",
    "### **Eigenvector Models**:\n",
    "- `test_max_eig.pt` - Model weights set to maximum eigenvector\n",
    "- `test_min_eig.pt` - Model weights set to minimum eigenvector  \n",
    "\n",
    "### **Loss Landscapes**:\n",
    "- `demo_loss_landscapes_df.pkl` - Structured DataFrame with landscapes and metadata\n",
    "- `demo_raw_loss_landscape_array.npy` - Raw 3D numpy array (samples × grid_x × grid_y)\n",
    "\n",
    "## **How to Use This Demo**\n",
    "\n",
    "1. **Run all cells sequentially** - The notebook is designed to be executed from top to bottom\n",
    "2. **Monitor memory usage** - Watch GPU/CPU memory during Hessian computation\n",
    "3. **Adjust parameters** - Modify grid size, sample count, or scaling factors as needed\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f91fb6",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll need several libraries for model loading, data processing, and Hessian computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import json\n",
    "import torch\n",
    "import copy\n",
    "from alignn.models.alignn import ALIGNN, ALIGNNConfig\n",
    "from collections import OrderedDict\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "from util.utils_AD import *\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.hessian_eigenvector import min_max_hessian_eigs, force_wts_into_model, npvec_to_tensorlist\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f09c6",
   "metadata": {},
   "source": [
    "## 2. Setup Device and Paths\n",
    "\n",
    "Configure the computational device and define paths to our demo data and model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be680d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths to demo data\n",
    "data_path = 'demo_JVDFT_dHf_dataset_50.pkl'\n",
    "model_path = 'demo_JVDFT_dHf_model.pt'\n",
    "target = 'formation_energy_peratom'  # Formation energy target\n",
    "\n",
    "print(f\"Data path: {data_path}\")\n",
    "print(f\"Model path: {model_path}\")\n",
    "print(f\"Target property: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa35f99",
   "metadata": {},
   "source": [
    "## 3. Load and Examine Dataset\n",
    "\n",
    "Load the demo dataset and examine its structure. This dataset contains 50 samples from JARVIS-DFT with formation energy values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4422aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_df = pd.read_pickle(data_path)\n",
    "print(f\"Dataset shape: {data_df.shape}\")\n",
    "print(f\"Columns: {list(data_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3bbe42",
   "metadata": {},
   "source": [
    "The dataset should be a pandas DataFrame with at least three columns: \n",
    "- **'jid'**: This column serves as an arbitrary identifier for the structure, which could be a formula or any identifier you prefer.\n",
    "- **'atoms'**: This is the input column, which should be created by converting your structure to a JARVIS Atoms structure and exporting it as a dictionary object.\n",
    "- **'formula**: The formula of the structure (optional).\n",
    "- A target label column of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c07b92",
   "metadata": {},
   "source": [
    "## 4. Load Trained ALIGNN Model\n",
    "\n",
    "Load the pre-trained ALIGNN model from the checkpoint file. This model has been trained on formation energy prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2674bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "checkpoint = torch.load(model_path, map_location=torch.device(device), weights_only=False)\n",
    "print(\"Checkpoint loaded successfully\")\n",
    "\n",
    "# Initialize ALIGNN model and load weights\n",
    "model = ALIGNN()\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded and moved to {device}\")\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "# Store model weights dictionary for later use\n",
    "model_wt_dict = OrderedDict([i for i in model.named_parameters()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef53931",
   "metadata": {},
   "source": [
    "## 5. Create Data Loader and Make Predictions\n",
    "\n",
    "Convert the dataset to the format expected by ALIGNN and create a data loader. Then make predictions to calculate errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb373c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to list format\n",
    "data_list = [row.to_dict() for _, row in data_df.iterrows()]\n",
    "print(f\"Converted {len(data_list)} samples to data list format\")\n",
    "\n",
    "# Create data loader\n",
    "data_loader = get_data_loader(data_list, target, workers=0)\n",
    "print(f\"Data loader created with {len(data_loader)} batches\")\n",
    "\n",
    "# Make predictions and calculate errors\n",
    "predictions = []\n",
    "true_values = []\n",
    "errors = []\n",
    "\n",
    "print(\"Making predictions...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        if i % 50 == 0:  # Progress indicator\n",
    "            print(f\"  Processing batch {i+1}/{len(data_loader)}\")\n",
    "        \n",
    "        s0, s1, target_batch = batch\n",
    "        s0, s1, target_batch = s0.to(device), s1.to(device), target_batch.to(device)\n",
    "        \n",
    "        # Make prediction\n",
    "        pred = model((s0, s1))\n",
    "        \n",
    "        # Store results\n",
    "        predictions.extend(pred.cpu().numpy().flatten())\n",
    "        true_values.extend(target_batch.cpu().numpy().flatten())\n",
    "        \n",
    "        # Calculate absolute errors\n",
    "        batch_errors = torch.abs(pred.flatten() - target_batch.flatten()).cpu().numpy()\n",
    "        errors.extend(batch_errors)\n",
    "\n",
    "print(f\"Predictions completed for {len(predictions)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae3867",
   "metadata": {},
   "source": [
    "## 6. Identify 20 Lowest Error Samples\n",
    "\n",
    "Since often times it is impossible to compute the Hessian eigenvectors for the full dataset due to the enormous amount of memory required. It is best to choose a subset to approximate the full dataset. One way to do it is to find the samples with the lowest prediction errors. These samples represent cases where the model performs best. Alternative ways could be random sampling, selection based on input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cccb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = data_df.copy()\n",
    "results_df['predicted'] = predictions\n",
    "results_df['true_value'] = true_values\n",
    "results_df['absolute_error'] = errors\n",
    "\n",
    "# Sort by error and get 20 lowest error samples\n",
    "lowest_error_df = results_df.nsmallest(20, 'absolute_error')\n",
    "\n",
    "print(\"Error Statistics:\")\n",
    "print(f\"Mean error: {np.mean(errors):.4f}\")\n",
    "print(f\"Median error: {np.median(errors):.4f}\")\n",
    "print(f\"Min error: {np.min(errors):.4f}\")\n",
    "print(f\"Max error: {np.max(errors):.4f}\")\n",
    "\n",
    "print(f\"\\n20 Lowest Error Samples:\")\n",
    "print(f\"Error range: {lowest_error_df['absolute_error'].min():.6f} - {lowest_error_df['absolute_error'].max():.6f}\")\n",
    "print(f\"JIDs: {list(lowest_error_df['jid'].values)}\")\n",
    "\n",
    "# Show the lowest error samples\n",
    "lowest_error_df[['jid', 'true_value', 'predicted', 'absolute_error']].head(10)\n",
    "lowest_error_df.to_pickle('demo_JVDFT_dHf_dataset_lowest_20_error_samples_from_50.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e91ebf",
   "metadata": {},
   "source": [
    "## 7. Prepare Data for Hessian Computation\n",
    "\n",
    "Create a data loader using only the 20 lowest error samples for Hessian eigenvector computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ead5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lowest error samples to list format\n",
    "lowest_error_list = [row.to_dict() for _, row in lowest_error_df.iterrows()]\n",
    "print(f\"Selected {len(lowest_error_list)} lowest error samples for Hessian computation\")\n",
    "\n",
    "# Create data loader for Hessian computation\n",
    "hessian_data_loader = get_data_loader(lowest_error_list, target, workers=0)\n",
    "print(f\"Hessian data loader created with {len(hessian_data_loader)} batches\")\n",
    "\n",
    "# Prepare model for Hessian computation\n",
    "loss_func = torch.nn.MSELoss()\n",
    "func = copy.deepcopy(model)\n",
    "func.to(device)\n",
    "func.eval()\n",
    "\n",
    "print(f\"Model prepared for Hessian computation on {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad695f",
   "metadata": {},
   "source": [
    "## 8. Compute Hessian Eigenvectors\n",
    "\n",
    "This is the computationally intensive step where we compute the maximum and minimum eigenvectors of the Hessian matrix. \n",
    "\n",
    "**Note**: This computation may take several minutes depending on your hardware and model size. \n",
    "\n",
    "You could adjust the amount of samples used to see its influence on the memory required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original model parameters for eigenvector conversion\n",
    "og_params = [i[1] for i in func.named_parameters() if len(i[1].size()) >= 1]\n",
    "og_layer_names = [i[0] for i in func.named_parameters() if len(i[1].size()) >= 1]\n",
    "\n",
    "print(f\"Model structure:\")\n",
    "print(f\"Total parameters with gradients: {len(og_params)}\")\n",
    "print(f\"Total trainable parameters: {sum(p.numel() for p in og_params)}\")\n",
    "\n",
    "print(f\"\\nStarting Hessian eigenvector computation...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Compute Hessian eigenvectors\n",
    "maxeig, mineig, maxeigvec, mineigvec, second_maxeig, second_maxeigvec = min_max_hessian_eigs(\n",
    "    func, hessian_data_loader, loss_func, \n",
    "    all_params=False, verbose=False, use_cuda=(device=='cuda')\n",
    ")\n",
    "\n",
    "print(f\"\\nHessian computation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d12e40",
   "metadata": {},
   "source": [
    "If the calculation is correct, you should get (+/- 0.1%):\n",
    "- Maximum eigenvalue: 16.390605\n",
    "- Minimum eigenvalue: 6.854265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if maxeig < mineig:\n",
    "    maxeig, mineig = mineig, maxeig\n",
    "    maxeigvec, mineigvec = mineigvec, maxeigvec\n",
    "    print(\"Assumption of minimum eigenvalue < 0 is false. Switched maximum and minimum eigenvalues and eigenvectors.\")\n",
    "\n",
    "print(f\"Maximum eigenvalue: {maxeig:.6f}\")\n",
    "print(f\"Minimum eigenvalue: {mineig:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca37837",
   "metadata": {},
   "source": [
    "## 9. Convert Eigenvectors to Model Weights\n",
    "\n",
    "Convert the computed eigenvectors back into the tensor format that matches the model's parameter structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert eigenvectors to model weight tensors\n",
    "print(\"Converting eigenvectors to model weight format...\")\n",
    "\n",
    "max_model_wts = npvec_to_tensorlist(maxeigvec, og_params)\n",
    "min_model_wts = npvec_to_tensorlist(mineigvec, og_params)\n",
    "\n",
    "print(f\"Max eigenvector converted to {len(max_model_wts)} weight tensors\")\n",
    "print(f\"Min eigenvector converted to {len(min_model_wts)} weight tensors\")\n",
    "\n",
    "# Verify shapes match\n",
    "print(f\"\\nShape verification:\")\n",
    "for i, (orig, max_eig, min_eig) in enumerate(zip(og_params, max_model_wts, min_model_wts)):\n",
    "    if i < 3:  # Show first 3 layers only\n",
    "        print(f\"  Layer {i}: Original {orig.shape} == Max {max_eig.shape} == Min {min_eig.shape}\")\n",
    "    if orig.shape != max_eig.shape or orig.shape != min_eig.shape:\n",
    "        print(f\"Shape mismatch at layer {i}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"All shapes match correctly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d216c50",
   "metadata": {},
   "source": [
    "## 10. Create and Save Eigenvector Models\n",
    "\n",
    "Create new model instances with weights set to the eigenvectors and save them for later use in loss landscape generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the original model\n",
    "model_eig_max = copy.deepcopy(func)\n",
    "model_eig_min = copy.deepcopy(func)\n",
    "\n",
    "print(\"Loading eigenvectors into model copies...\")\n",
    "\n",
    "# Load eigenvectors into models\n",
    "model_eig_max = force_wts_into_model(og_layer_names, max_model_wts, model_eig_max, model_wt_dict)\n",
    "model_eig_min = force_wts_into_model(og_layer_names, min_model_wts, model_eig_min, model_wt_dict)\n",
    "\n",
    "print(\"Eigenvectors loaded into models successfully\")\n",
    "\n",
    "# Save the eigenvector models\n",
    "print(\"Saving eigenvector models...\")\n",
    "\n",
    "os.makedirs('demo_computed_eigenvectors', exist_ok=True)\n",
    "torch.save(model_eig_max.state_dict(), 'demo_computed_eigenvectors/test_max_eig.pt')\n",
    "torch.save(model_eig_min.state_dict(), 'demo_computed_eigenvectors/test_min_eig.pt')\n",
    "\n",
    "print(\"Models saved:\")\n",
    "print(\"  - test_max_eig.pt (maximum eigenvector model)\")\n",
    "print(\"  - test_min_eig.pt (minimum eigenvector model)\")\n",
    "\n",
    "# Also save eigenvalues for reference\n",
    "eigenvalue_info = {\n",
    "    'max_eigenvalue': float(maxeig),\n",
    "    'min_eigenvalue': float(mineig),\n",
    "    'second_max_eigenvalue': float(second_maxeig),\n",
    "    'num_samples': len(lowest_error_list),\n",
    "    'target': target\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a87bd",
   "metadata": {},
   "source": [
    "# Part 2: Loss Landscape Generation\n",
    "\n",
    "Now that we have computed the Hessian eigenvectors, let's continue to generate the actual 2D loss landscapes. We'll use the eigenvector models we just created to explore the loss surface around our trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356228f5",
   "metadata": {},
   "source": [
    "## 12. Import Additional Libraries for Loss Landscapes\n",
    "\n",
    "We need the loss_landscapes library for generating the 2D interpolation grids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb13703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_landscapes\n",
    "import loss_landscapes.metrics\n",
    "from loss_landscapes.model_interface.model_wrapper import ModelWrapper\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "print(\"Loss landscapes libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476db87",
   "metadata": {},
   "source": [
    "## 13. Define Custom Loss Metric\n",
    "\n",
    "Create a custom loss metric class that will be used to evaluate the model at different points in the loss landscape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a312019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(ABC):\n",
    "    \"\"\" A quantity that can be computed given a model or an agent. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, model_wrapper: ModelWrapper):\n",
    "        pass\n",
    "\n",
    "class Loss(Metric):\n",
    "    \"\"\" Computes a specified loss function over specified input-output pairs. \"\"\"\n",
    "    def __init__(self, loss_fn, model, inputs: torch.Tensor, target: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.loss_fn = loss_fn\n",
    "        self.inputs = inputs\n",
    "        self.model = model\n",
    "        self.target = target\n",
    "\n",
    "    def __call__(self, model_wrapper: ModelWrapper) -> float:\n",
    "        outputs = model_wrapper.forward(self.inputs)\n",
    "        err = self.loss_fn(self.target[0], outputs)\n",
    "        return err\n",
    "\n",
    "def split_3d_array(array):\n",
    "    return [array[:,:,i:i+1] for i in range(array.shape[2])]\n",
    "\n",
    "print(\"Custom metric classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f00ba",
   "metadata": {},
   "source": [
    "## 14. Load Saved Eigenvector Models\n",
    "\n",
    "Load the eigenvector models we just saved to use as the perturbation directions for our loss landscape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceea2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the eigenvector models we just created\n",
    "print(\"Loading saved eigenvector models...\")\n",
    "\n",
    "# Create fresh model copies for loss landscape computation\n",
    "model_eig_max_ll = copy.deepcopy(model)\n",
    "model_eig_min_ll = copy.deepcopy(model)\n",
    "\n",
    "# Load the saved eigenvector weights\n",
    "model_eig_max_ll.load_state_dict(torch.load('demo_computed_eigenvectors/test_max_eig.pt', weights_only=True))\n",
    "model_eig_min_ll.load_state_dict(torch.load('demo_computed_eigenvectors/test_min_eig.pt', weights_only=True))\n",
    "\n",
    "# Move models to device and set to eval mode\n",
    "model_eig_max_ll.to(device)\n",
    "model_eig_min_ll.to(device)\n",
    "model_eig_max_ll.eval()\n",
    "model_eig_min_ll.eval()\n",
    "\n",
    "print(\"Eigenvector models loaded successfully!\")\n",
    "print(f\"All models are on {device} and in eval mode\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98dac8f",
   "metadata": {},
   "source": [
    "## 15. Create Loss Metrics for Each Sample\n",
    "\n",
    "Create loss metric objects for the all 50 samples. Each metric will evaluate the loss for one specific sample. Here we use the MSE loss as the metric. \n",
    "\n",
    "Other viable metrics include:\n",
    "1. MAE\n",
    "2. Huber Loss\n",
    "3. LogCosh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics for each batch in our lowest error dataset\n",
    "print(\"Creating loss metrics for each sample...\")\n",
    "\n",
    "metric_list = []\n",
    "sample_count = 0\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for batch in data_loader:\n",
    "    s0_device = batch[0].to(device)\n",
    "    s1_device = batch[1].to(device)\n",
    "    s2_device = batch[2].to(device)\n",
    "    \n",
    "    x_train = (s0_device, s1_device)\n",
    "    y_train = (s2_device)\n",
    "    \n",
    "    # Create a loss metric for this batch\n",
    "    metric_list.append(Loss(loss_func, model.eval(), x_train, y_train))\n",
    "    sample_count += len(s2_device)\n",
    "\n",
    "print(f\"Created {len(metric_list)} loss metrics for {sample_count} samples\")\n",
    "print(f\"Each metric will evaluate loss for one batch of data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdfdf5c",
   "metadata": {},
   "source": [
    "## 16. Configure Loss Landscape Parameters\n",
    "\n",
    "Set up the parameters for generating the 2D loss landscape grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361400f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure loss landscape parameters\n",
    "steps = 20  # Creates a 20x20 grid (you can adjust this)\n",
    "scale_factor = 1.0  # Scaling factor for eigenvector perturbations\n",
    "half = False  # Set to True to skip every other computation for speed\n",
    "\n",
    "print(f\"Loss landscape configuration:\")\n",
    "print(f\"  Grid size: {steps} x {steps}\")\n",
    "print(f\"  Scale factor: {scale_factor}\")\n",
    "print(f\"  Half computation: {half}\")\n",
    "print(f\"  Total evaluations: {steps * steps * len(metric_list) if not half else steps * steps * len(metric_list) // 2}\")\n",
    "\n",
    "# Estimate computation time\n",
    "total_evals = steps * steps * len(metric_list)\n",
    "if half:\n",
    "    total_evals = total_evals // 2\n",
    "\n",
    "print(f\"Estimated total model evaluations: {total_evals:,}\")\n",
    "print(f\"This may take several minutes depending on your hardware...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b737f3f",
   "metadata": {},
   "source": [
    "## 17. Generate 2D Loss Landscapes\n",
    "\n",
    "Now we'll generate the actual loss landscapes by interpolating between the original model and the two eigenvector models.\n",
    "\n",
    "**Note**: This is may take several minutes to complete but is not memory intensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate loss landscapes using batch planar interpolation\n",
    "print(\"Starting loss landscape generation...\")\n",
    "print(\"This will create a 2D grid by interpolating between:\")\n",
    "print(\"  - Original model (center)\")\n",
    "print(\"  - Maximum eigenvector model (axis 1)\")\n",
    "print(\"  - Minimum eigenvector model (axis 2)\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    loss_data_fin = loss_landscapes.batch_planar_interpolation(\n",
    "        model_start=model.eval(),\n",
    "        model_end_one=model_eig_max_ll.eval(),\n",
    "        model_end_two=model_eig_min_ll.eval(),\n",
    "        metric_list=metric_list,\n",
    "        steps=steps,\n",
    "        deepcopy_model=True,\n",
    "        scale=scale_factor,\n",
    "        half=half\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Loss landscape generation completed!\")\n",
    "    print(f\"Total computation time: {computation_time:.2f} seconds\")\n",
    "    print(f\"Result shape: {loss_data_fin.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during loss landscape generation:\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d007ea1e",
   "metadata": {},
   "source": [
    "## 18. Process and Organize Results\n",
    "\n",
    "Split the 3D result array and create a structured DataFrame for easy analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the results\n",
    "print(\"Processing loss landscape results...\")\n",
    "\n",
    "# Split the 3D array into individual 2D landscapes for each sample\n",
    "landscapes_list = split_3d_array(loss_data_fin)\n",
    "\n",
    "print(f\"Processing results:\")\n",
    "print(f\"  Original shape: {loss_data_fin.shape}\")\n",
    "print(f\"  Number of individual landscapes: {len(landscapes_list)}\")\n",
    "print(f\"  Each landscape shape: {landscapes_list[0].shape}\")\n",
    "\n",
    "# Create a results DataFrame\n",
    "loss_landscapes_df = pd.DataFrame()\n",
    "loss_landscapes_df['jid'] = data_df['jid'].values\n",
    "loss_landscapes_df['raw_loss_landscapes'] = landscapes_list\n",
    "\n",
    "print(f\"Created results DataFrame with {len(loss_landscapes_df)} samples\")\n",
    "print(f\"DataFrame columns: {list(loss_landscapes_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1dfae",
   "metadata": {},
   "source": [
    "## 19. Save Results\n",
    "\n",
    "Save the loss landscape results in multiple formats for future analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "print(\"Saving loss landscape results...\")\n",
    "\n",
    "os.makedirs('demo_computed_landscapes', exist_ok=True)\n",
    "\n",
    "# Save DataFrame as pickle\n",
    "loss_landscapes_df.to_pickle('demo_computed_landscapes/demo_loss_landscapes_df.pkl')\n",
    "print(\"Saved: demo_loss_landscapes_df.pkl\")\n",
    "\n",
    "# Save raw array as numpy file\n",
    "np.save('demo_computed_landscapes/demo_raw_loss_landscape_array.npy', loss_data_fin)\n",
    "print(\"Saved: demo_raw_loss_landscape_array.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad54d3",
   "metadata": {},
   "source": [
    "## 20. Visualize Sample Loss Landscapes\n",
    "\n",
    "Let's create some sample visualizations to see what the loss landscapes look like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for the first few samples\n",
    "print(\"Creating sample loss landscape visualizations...\")\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "center = steps // 2\n",
    "\n",
    "# Plot the first 6 samples\n",
    "for i in range(min(6, len(landscapes_list))):\n",
    "    landscape = landscapes_list[i][:, :, 0]  # Remove the singleton dimension\n",
    "    jid = loss_landscapes_df.iloc[i]['jid']  \n",
    "\n",
    "    # Create contour plot\n",
    "    im = axes[i].imshow(np.log(landscape), cmap='viridis', origin='lower', extent=[-center, center, -center, center])\n",
    "    axes[i].set_title(f'Sample {i+1}: {jid}', fontsize=10)\n",
    "    axes[i].set_xlabel('Max Eigenvector Direction')\n",
    "    axes[i].set_ylabel('Min Eigenvector Direction')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=axes[i], shrink=0.8)\n",
    "    cbar.set_label('log(MSE error)', fontsize=8)\n",
    "    \n",
    "    # Mark the center point (original model)\n",
    "    axes[i].plot(0, 0, 'r*', markersize=10, label='Original Model')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Loss Landscapes for Lowest Error Samples', fontsize=16, y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average landscape\n",
    "avg_landscape = np.mean([landscape[:, :, 0] for landscape in landscapes_list], axis=0)\n",
    "\n",
    "# Set up the plot for the average landscape\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Average Loss Landscape', fontsize=14)\n",
    "plt.xlabel(f'Max Eigenvector Direction (max eigenvalue: {maxeig:.6f})')\n",
    "plt.ylabel(f'Min Eigenvector Direction (min eigenvalue: {mineig:.6f})')\n",
    "\n",
    "# Create contour plot for the average landscape\n",
    "im = plt.imshow(np.log(avg_landscape), cmap='viridis', origin='lower', extent=[-center, center, -center, center])\n",
    "cbar = plt.colorbar(im, shrink=0.8)\n",
    "cbar.set_label('log(MSE error)', fontsize=10)\n",
    "\n",
    "# Mark the center point (original model)\n",
    "plt.plot(0, 0, 'r*', markersize=10, label='Original Model')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Sample visualizations created!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "losslandscapefeb8gpu4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
