{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73235bc0",
   "metadata": {},
   "source": [
    "# Demo 3: Loss Landscape Post-Processing and Feature Extraction\n",
    "\n",
    "This notebook demonstrates how to **post-process loss landscape data** to extract meaningful features and statistics for machine learning analysis. We'll show both the **manual step-by-step approach** and the **automated script method**.\n",
    "\n",
    "## **What This Demo Covers**\n",
    "\n",
    "This notebook teaches you how to:\n",
    "1. **Load and inspect loss landscape data** from previous demos\n",
    "2. **Extract chemical and structural features** using matminer\n",
    "3. **Compute loss landscape metrics** (origin loss, roughness, optimality, etc.)\n",
    "4. **Apply statistical transformations** (normalization, standardization)\n",
    "5. **Save processed data** for machine learning analysis\n",
    "6. **Use automated post-processing scripts** with configuration files\n",
    "\n",
    "## **Expected Input Data**\n",
    "\n",
    "This demo expects loss landscape data generated from **Demo 1** or **Demo 2**:\n",
    "\n",
    "### **Required Input Files:**\n",
    "- `computed_loss_landscapes/demo2_automated_landscape/loss_landscapes_df.pkl` - Loss landscape DataFrame\n",
    "- `computed_loss_landscapes/demo2_automated_landscape/config.yml` - Original experiment configuration\n",
    "- `demo/demo_JVDFT_dHf_dataset_50.pkl` - Original dataset with material information\n",
    "\n",
    "## **Expected Generated Files**\n",
    "\n",
    "This post-processing workflow will create several analysis-ready files:\n",
    "\n",
    "### **Manual Processing Results:**\n",
    "- Various intermediate DataFrames with computed features\n",
    "- Statistical metrics for each loss landscape\n",
    "- Normalized and standardized data arrays\n",
    "\n",
    "### **Automated Script Results (`computed_loss_landscapes/demo2_automated_landscape/`):**\n",
    "- `feat_sample_df.pkl` - Enhanced sample data with additional features\n",
    "- `feat_sample_composition_df.pkl` - Chemical composition features (optional)\n",
    "- `feat_sample_structure_df.pkl` - Crystal structure features (optional)  \n",
    "- `processed_loss_function_dict.pkl` - Complete loss landscape metrics and statistics\n",
    "\n",
    "## **Key Processing Steps**\n",
    "\n",
    "1. **Loss Landscape Metrics**: Origin loss, average loss, standard deviation, optimality checks\n",
    "2. **Chemical Features**: Elemental properties, stoichiometry, valence orbitals (optional)\n",
    "3. **Structural Features**: Density, symmetry features (optional)\n",
    "4. **Statistical Transformations**: Log transforms, z-score normalization, min-max scaling\n",
    "\n",
    "## **Script We'll Use**\n",
    "\n",
    "- **`post_process_loss_landscapes.py`** - Automated post-processing from config file\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started with the manual approach first!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c08392",
   "metadata": {},
   "source": [
    "# Part 1: Manual Step-by-Step Post-Processing\n",
    "\n",
    "## 1. Import Libraries and Setup\n",
    "\n",
    "First, let's import all the necessary libraries for data processing, feature extraction, and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir('..') # Change working directory to the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Any\n",
    "\n",
    "# For loss landscape processing\n",
    "from util.landscape_processing import (\n",
    "    extract_loss_landscapes,\n",
    "    apply_function_to_column_and_add,\n",
    "    loss_at_origin,\n",
    "    average_loss,\n",
    "    standard_deviation_of_loss,\n",
    "    is_original_loss_the_lowest,\n",
    "    lowest_over_original_loss,\n",
    "    euclidean_distance_best_to_original,\n",
    "    log_of_array,\n",
    "    z_transform_standardize,\n",
    "    min_max_normalize\n",
    ")\n",
    "\n",
    "# For sample featurizing (optional chemical/structural features)\n",
    "from util.sample_featurizing import get_pymatgen_structures_to_df, add_num_elements_column\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Note: Matminer features are optional and will be shown later\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa2e48",
   "metadata": {},
   "source": [
    "## 2. Load Loss Landscape Data\n",
    "\n",
    "Let's load the loss landscape data generated from Demo 2 (or Demo 1) and examine its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ab7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load loss landscape data from Demo 2\n",
    "landscape_folder = os.path.join(\"computed_loss_landscapes\", \"demo2_automated_landscape\")\n",
    "landscape_pkl_path = os.path.join(landscape_folder, \"loss_landscapes_df.pkl\")\n",
    "config_path = os.path.join(landscape_folder, \"config.yml\")\n",
    "\n",
    "# Check if files exist\n",
    "if os.path.exists(landscape_pkl_path):\n",
    "    print(f\"Found loss landscape data: {landscape_pkl_path}\")\n",
    "    loss_landscapes_df = pd.read_pickle(landscape_pkl_path)\n",
    "    print(f\"Loaded DataFrame shape: {loss_landscapes_df.shape}\")\n",
    "    print(f\"Columns: {list(loss_landscapes_df.columns)}\")\n",
    "else:\n",
    "    print(f\"Loss landscape file not found: {landscape_pkl_path}\")\n",
    "    print(\"Please run Demo 2 first to generate loss landscape data.\")\n",
    "\n",
    "# Load original experiment config\n",
    "if os.path.exists(config_path):\n",
    "    print(f\"\\nFound experiment config: {config_path}\")\n",
    "    with open(config_path, 'r') as f:\n",
    "        expt_config = yaml.safe_load(f)\n",
    "    print(\"Configuration:\")\n",
    "    for key, value in expt_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(f\"Config file not found: {config_path}\")\n",
    "\n",
    "# Load original dataset\n",
    "original_data_path = expt_config['data_path']\n",
    "if os.path.exists(original_data_path):\n",
    "    print(f\"\\nLoading original dataset: {original_data_path}\")\n",
    "    sample_df = pd.read_pickle(original_data_path)\n",
    "    print(f\"Original dataset shape: {sample_df.shape}\")\n",
    "    print(f\"Original dataset columns: {list(sample_df.columns)}\")\n",
    "else:\n",
    "    print(f\"Original dataset not found: {original_data_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b8cda",
   "metadata": {},
   "source": [
    "## 3. Examine Loss Landscape Structure\n",
    "\n",
    "Let's examine the structure of the loss landscape data and understand what we're working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3020ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the loss landscape data structure\n",
    "print(\"Loss Landscape Data Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Look at the first few rows\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(loss_landscapes_df.head())\n",
    "\n",
    "# Examine a sample loss landscape array\n",
    "print(f\"\\nSample loss landscape analysis:\")\n",
    "sample_landscape = loss_landscapes_df['raw_loss_landscapes'].iloc[0]\n",
    "print(f\"Individual landscape shape: {sample_landscape.shape}\")\n",
    "print(f\"Data type: {sample_landscape.dtype}\")\n",
    "print(f\"Value range: {sample_landscape.min():.6f} to {sample_landscape.max():.6f}\")\n",
    "\n",
    "# Get the 2D landscape (remove singleton dimension)\n",
    "landscape_2d = sample_landscape[:, :, 0]\n",
    "center_idx = landscape_2d.shape[0] // 2\n",
    "center_loss = landscape_2d[center_idx, center_idx]\n",
    "min_loss = landscape_2d.min()\n",
    "max_loss = landscape_2d.max()\n",
    "\n",
    "print(f\"\\n2D Landscape Analysis:\")\n",
    "print(f\"Grid size: {landscape_2d.shape}\")\n",
    "print(f\"Center loss (original model): {center_loss:.6f}\")\n",
    "print(f\"Minimum loss in grid: {min_loss:.6f}\")\n",
    "print(f\"Maximum loss in grid: {max_loss:.6f}\")\n",
    "print(f\"Loss range: {max_loss - min_loss:.6f}\")\n",
    "\n",
    "# Create a simple visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "im = plt.imshow(sample_landscape, cmap='viridis', origin='lower', extent=[-center_idx, center_idx, -center_idx, center_idx])\n",
    "plt.colorbar(label='Loss Value')\n",
    "plt.title(f'Sample Loss Landscape: {loss_landscapes_df[\"jid\"].iloc[0]}')\n",
    "plt.xlabel('Max Eigenvector Direction')\n",
    "plt.ylabel('Min Eigenvector Direction')\n",
    "plt.plot(0, 0, 'r*', markersize=15, label='Original Model')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f76de",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Processing\n",
    "\n",
    "We need to rename the loss landscape column and extract the landscapes into the expected format for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the loss landscape data for processing\n",
    "run_id = expt_config['run_id']\n",
    "print(f\"Run ID: {run_id}\")\n",
    "\n",
    "# Rename the column to include the run_id (this is what the processing functions expect)\n",
    "loss_landscapes_processed = loss_landscapes_df.rename(\n",
    "    columns={'raw_loss_landscapes': f'{run_id}_mse_loss_landscape_array'}\n",
    ")\n",
    "\n",
    "print(f\"Renamed column to: {run_id}_mse_loss_landscape_array\")\n",
    "print(f\"Processed DataFrame columns: {list(loss_landscapes_processed.columns)}\")\n",
    "\n",
    "# Extract loss landscapes into the expected dictionary format\n",
    "print(f\"\\nExtracting loss landscapes...\")\n",
    "loss_function_dict = extract_loss_landscapes(loss_landscapes_processed)\n",
    "\n",
    "# Examine the extracted data\n",
    "for loss_function, df in loss_function_dict.items():\n",
    "    print(f\"\\nLoss function: {loss_function}\")\n",
    "    print(f\"  DataFrame shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Sample landscape array shape: {df.iloc[0, 1].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c81e09",
   "metadata": {},
   "source": [
    "## 5. Compute Loss Landscape Metrics\n",
    "\n",
    "Now let's compute various metrics from the loss landscapes that characterize their properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832afa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute various loss landscape metrics\n",
    "print(\"Computing Loss Landscape Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Work with the first (and only) loss function in our dictionary\n",
    "loss_function = list(loss_function_dict.keys())[0]\n",
    "df = loss_function_dict[loss_function].copy()\n",
    "\n",
    "print(f\"Processing loss function: {loss_function}\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "\n",
    "# Define the metrics to compute\n",
    "metrics = [\n",
    "    (loss_at_origin, \"loss_at_origin\", \"Loss value at the original model (center point)\"),\n",
    "    (average_loss, \"average_of_landscape\", \"Average loss across the entire landscape\"),\n",
    "    (standard_deviation_of_loss, \"std_dev_of_landscape\", \"Standard deviation of loss values\"),\n",
    "    (is_original_loss_the_lowest, \"is_original_loss_lowest\", \"Whether original model is optimal\"),\n",
    "    (lowest_over_original_loss, \"lowest_loss_over_original\", \"Ratio of minimum to original loss\"),\n",
    "    (euclidean_distance_best_to_original, \"euclidean_distance_best_to_original\", \"Distance from original to optimal point\"),\n",
    "    (log_of_array, \"log_loss_landscape_array\", \"Log-transformed loss landscape\")\n",
    "]\n",
    "\n",
    "print(f\"\\nComputing {len(metrics)} metrics for each landscape...\")\n",
    "\n",
    "# Apply each metric function\n",
    "for i, (metric_func, column_name, description) in enumerate(metrics):\n",
    "    print(f\"  {i+1}. Computing {column_name}...\")\n",
    "    df = apply_function_to_column_and_add(\n",
    "        df, metric_func, \"loss_landscape_array\", column_name, loss_function\n",
    "    )\n",
    "\n",
    "print(f\"\\nCompleted metric computation!\")\n",
    "print(f\"DataFrame now has {len(df.columns)} columns:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Display some sample results\n",
    "print(f\"\\nSample metric values (first 5 samples):\")\n",
    "metric_columns = [col for col in df.columns if any(metric[1] in col for metric in metrics)]\n",
    "sample_metrics = df[['jid'] + metric_columns].head()\n",
    "sample_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13033a68",
   "metadata": {},
   "source": [
    "## 6. Apply Statistical Transformations\n",
    "\n",
    "Let's apply normalization and standardization to make the data suitable for machine learning analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4316143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply statistical transformations\n",
    "print(\"Applying Statistical Transformations:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Apply z-score standardization to log-transformed arrays\n",
    "print(\"1. Applying z-score standardization to log-transformed landscapes...\")\n",
    "df = z_transform_standardize(df, \"log_loss_landscape_array\", loss_function)\n",
    "\n",
    "# Apply min-max normalization to log-transformed arrays  \n",
    "print(\"2. Applying min-max normalization to log-transformed landscapes...\")\n",
    "df = min_max_normalize(df, \"log_loss_landscape_array\", loss_function)\n",
    "\n",
    "print(\"Statistical transformations completed!\")\n",
    "\n",
    "# Check what new columns were added\n",
    "new_columns = [col for col in df.columns if 'z_transform' in col or 'min_max' in col]\n",
    "print(f\"\\nNew transformation columns added:\")\n",
    "for col in new_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Update the loss function dictionary\n",
    "loss_function_dict[loss_function] = df\n",
    "\n",
    "print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Show a summary of the metric values\n",
    "print(f\"\\nMetric Value Summary:\")\n",
    "numeric_columns = [col for col in df.columns if col not in ['jid'] and not any(x in col for x in ['array', 'log_loss'])]\n",
    "for col in numeric_columns:\n",
    "    if df[col].dtype in ['float64', 'int64', 'bool']:\n",
    "        if df[col].dtype == 'bool':\n",
    "            print(f\"  {col}: {df[col].sum()}/{len(df)} samples are True\")\n",
    "        else:\n",
    "            print(f\"  {col}: {df[col].mean():.4f} ± {df[col].std():.4f}\")\n",
    "    else:\n",
    "        print(f\"  {col}: {df[col].dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf4331",
   "metadata": {},
   "source": [
    "## 7. Enhance Sample Data\n",
    "\n",
    "Let's add some basic features to the original sample data to prepare it for the complete analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c22be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance the sample data with additional features\n",
    "print(\"Enhancing Sample Data:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Add number of elements column (a simple structural feature)\n",
    "print(\"Adding number of elements feature...\")\n",
    "sample_df_enhanced = add_num_elements_column(sample_df.copy())\n",
    "\n",
    "print(f\"Enhanced sample DataFrame shape: {sample_df_enhanced.shape}\")\n",
    "print(f\"New columns: {list(sample_df_enhanced.columns)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nSample data with number of elements:\")\n",
    "print(sample_df_enhanced[['jid', 'formation_energy_peratom', 'num_elements']].head())\n",
    "\n",
    "# Analyze the number of elements distribution\n",
    "print(f\"\\nNumber of elements distribution:\")\n",
    "element_counts = sample_df_enhanced['num_elements'].value_counts().sort_index()\n",
    "for num_elem, count in element_counts.items():\n",
    "    print(f\"  {num_elem} elements: {count} samples\")\n",
    "\n",
    "print(f\"\\nBasic enhanced sample data is ready!\")\n",
    "print(f\"Note: Chemical composition and crystal structure features require matminer, \\nwhich is not shown here but is available in the full script.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12cee4",
   "metadata": {},
   "source": [
    "## 8. Visualize Key Metrics\n",
    "\n",
    "Let's create some visualizations to understand the computed loss landscape metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations of the key metrics\n",
    "print(\"Creating Loss Landscape Metric Visualizations:\")\n",
    "\n",
    "# Get the processed DataFrame\n",
    "df_viz = loss_function_dict[loss_function]\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define metrics to visualize\n",
    "viz_metrics = [\n",
    "    (f'{loss_function}_loss_at_origin', 'Loss at Origin'),\n",
    "    (f'{loss_function}_average_of_landscape', 'Average Loss'),\n",
    "    (f'{loss_function}_std_dev_of_landscape', 'Loss Standard Deviation'),\n",
    "    (f'{loss_function}_lowest_loss_over_original', 'Min/Original Loss Ratio'),\n",
    "    (f'{loss_function}_euclidean_distance_best_to_original', 'Distance to Optimum'),\n",
    "    (f'{loss_function}_is_original_loss_lowest', 'Original is Optimal (%)'),\n",
    "]\n",
    "\n",
    "# Create plots\n",
    "for i, (metric_col, title) in enumerate(viz_metrics):\n",
    "    if i < len(axes):\n",
    "        if 'is_original_loss_lowest' in metric_col:\n",
    "            # For boolean data, show percentage\n",
    "            pct_true = df_viz[metric_col].mean() * 100\n",
    "            axes[i].bar(['False', 'True'], \n",
    "                       [100-pct_true, pct_true], \n",
    "                       color=['red', 'green'], alpha=0.7)\n",
    "            axes[i].set_ylabel('Percentage (%)')\n",
    "            axes[i].set_title(f'{title}\\n({pct_true:.1f}% are optimal)')\n",
    "        else:\n",
    "            # For numeric data, show histogram\n",
    "            axes[i].hist(df_viz[metric_col], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[i].set_xlabel('Value')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].set_title(f'{title}\\n(μ={df_viz[metric_col].mean():.4f}, σ={df_viz[metric_col].std():.4f})')\n",
    "        \n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Loss Landscape Metrics Distribution', fontsize=16, y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb06c2a",
   "metadata": {},
   "source": [
    "# Part 2: Automated Post-Processing with Script\n",
    "\n",
    "Now let's see how to accomplish the same post-processing using the automated script.\n",
    "\n",
    "## 9. Create Configuration for Automated Post-Processing\n",
    "\n",
    "We need to create a configuration file for the automated post-processing script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f19da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for automated post-processing\n",
    "print(\"Creating Post-Processing Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Configuration for post-processing\n",
    "post_process_config = {\n",
    "    'folder': landscape_folder,  # The folder containing loss landscape results\n",
    "    'featurize': 'True',  # Set to 'True' to enable chemical/structural features\n",
    "    'n_jobs': 4,  # Number of parallel jobs for featurization\n",
    "    'output_format': 'pkl',  # Output format\n",
    "    'verbose': True  # Enable detailed logging\n",
    "}\n",
    "\n",
    "# Save configuration file\n",
    "config_file_path = os.path.join('demo', 'demo3_post_process_config.yml')\n",
    "\n",
    "with open(config_file_path, 'w') as f:\n",
    "    yaml.dump(post_process_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\nConfiguration saved to: {config_file_path}\")\n",
    "\n",
    "# Display the generated YAML file\n",
    "print(f\"\\nContents of {config_file_path}:\")\n",
    "print(\"-\" * 40)\n",
    "with open(config_file_path, 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(\"\\nConfiguration file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be99c1",
   "metadata": {},
   "source": [
    "### Configuration Explanation\n",
    "\n",
    "**Key Parameters for Post-Processing:**\n",
    "\n",
    "- **`folder`**: Path to the directory containing loss landscape results (must contain .pkl and .yml files)\n",
    "- **`featurize`**: String 'True'/'False' - whether to compute chemical/structural features using matminer\n",
    "- **`n_jobs`**: Number of CPU cores to use for parallel feature computation  \n",
    "- **`output_format`**: Output file format (currently only 'pkl' supported)\n",
    "- **`verbose`**: Enable detailed progress logging\n",
    "\n",
    "**Important Notes:**\n",
    "- Setting `featurize: 'True'` requires matminer library and significantly increases computation time\n",
    "- All results are saved back to the same folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae1bc2",
   "metadata": {},
   "source": [
    "## 10. Execute Automated Post-Processing Script\n",
    "\n",
    "Now let's run the automated post-processing script with our configuration.\n",
    "\n",
    "```bash\n",
    "python post_process_loss_landscapes.py demo3_post_process_config.yml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daabaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(\"Starting automated post-processing...\")\n",
    "print(\"This may take 1-2 minutes depending on the dataset size and featurization settings.\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Run the post-processing script\n",
    "    cmd = ['python', 'post_process_loss_landscapes.py', config_file_path]\n",
    "    print(f\"\\nExecuting: {' '.join(cmd)}\")\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)  # 10 min timeout\n",
    "    \n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\nPost-processing completed successfully!\")\n",
    "        print(f\"Total time: {computation_time:.2f} seconds ({computation_time/60:.2f} minutes)\")\n",
    "        print(f\"\\nScript output:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(f\"\\nError occurred during post-processing:\")\n",
    "        print(f\"Return code: {result.returncode}\")\n",
    "        print(f\"Error output: {result.stderr}\")\n",
    "        print(f\"Standard output: {result.stdout}\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(f\"\\nScript timed out after 10 minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nUnexpected error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541789e",
   "metadata": {},
   "source": [
    "## 11. Verify Automated Post-Processing Results\n",
    "\n",
    "Let's examine what files were created by the automated post-processing script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90714c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the automated post-processing results\n",
    "print(\"Checking Automated Post-Processing Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Expected output files from the automated script\n",
    "expected_files = [\n",
    "    'feat_sample_df.pkl',\n",
    "    'feat_sample_composition_df.pkl', \n",
    "    'feat_sample_structure_df.pkl',\n",
    "    'processed_loss_function_dict.pkl'\n",
    "]\n",
    "\n",
    "print(f\"Checking output directory: {landscape_folder}\")\n",
    "\n",
    "# Check what files were created\n",
    "created_files = []\n",
    "for filename in expected_files:\n",
    "    file_path = os.path.join(landscape_folder, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        created_files.append(filename)\n",
    "        print(f\"  Found: {filename} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"  Missing: {filename}\")\n",
    "\n",
    "print(f\"\\nSuccessfully created {len(created_files)}/{len(expected_files)} expected files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the processed loss function dictionary\n",
    "processed_dict_path = os.path.join(landscape_folder, 'processed_loss_function_dict.pkl')\n",
    "if os.path.exists(processed_dict_path):\n",
    "    print(f\"\\nLoading processed loss function dictionary...\")\n",
    "    \n",
    "    with open(processed_dict_path, 'rb') as f:\n",
    "        import pickle\n",
    "        automated_loss_dict = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loss functions processed: {list(automated_loss_dict.keys())}\")\n",
    "    \n",
    "    for loss_func, processed_df in automated_loss_dict.items():\n",
    "        print(f\"\\nLoss function: {loss_func}\")\n",
    "        print(f\"  DataFrame shape: {processed_df.shape}\")\n",
    "        print(f\"  Columns ({len(processed_df.columns)}):\")\n",
    "        for col in processed_df.columns:\n",
    "            print(f\"    - {col}\")\n",
    "            \n",
    "        # Compare with our manual results\n",
    "        if loss_func in loss_function_dict:\n",
    "            manual_df = loss_function_dict[loss_func]\n",
    "            print(f\"\\n  Comparison with manual processing:\")\n",
    "            print(f\"    Manual shape: {manual_df.shape}\")\n",
    "            print(f\"    Automated shape: {processed_df.shape}\")\n",
    "            print(f\"    Columns match: {set(manual_df.columns) == set(processed_df.columns)}\")\n",
    "\n",
    "# Load enhanced sample data\n",
    "sample_data_path = os.path.join(landscape_folder, 'feat_sample_df.pkl')\n",
    "if os.path.exists(sample_data_path):\n",
    "    print(f\"\\nLoading enhanced sample data...\")\n",
    "    \n",
    "    with open(sample_data_path, 'rb') as f:\n",
    "        automated_sample_df = pickle.load(f)\n",
    "    \n",
    "    print(f\"Enhanced sample data shape: {automated_sample_df.shape}\")\n",
    "    print(f\"Columns: {list(automated_sample_df.columns)}\")\n",
    "\n",
    "# Load and examine the feat_sample_composition_df\n",
    "composition_data_path = os.path.join(landscape_folder, 'feat_sample_composition_df.pkl')\n",
    "if os.path.exists(composition_data_path):\n",
    "    print(f\"\\nLoading sample composition data...\")\n",
    "    \n",
    "    with open(composition_data_path, 'rb') as f:\n",
    "        feat_sample_composition_df = pickle.load(f)\n",
    "    \n",
    "    print(f\"Sample composition data shape: {feat_sample_composition_df.shape}\")\n",
    "    print(f\"Columns: {list(feat_sample_composition_df.columns)}\")\n",
    "\n",
    "# Load and examine the feat_sample_structure_df\n",
    "structure_data_path = os.path.join(landscape_folder, 'feat_sample_structure_df.pkl')\n",
    "if os.path.exists(structure_data_path):\n",
    "    print(f\"\\nLoading sample structure data...\")\n",
    "    \n",
    "    with open(structure_data_path, 'rb') as f:\n",
    "        feat_sample_structure_df = pickle.load(f)\n",
    "    \n",
    "    print(f\"Sample structure data shape: {feat_sample_structure_df.shape}\")\n",
    "    print(f\"Columns: {list(feat_sample_structure_df.columns)}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAutomated post-processing verification completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACSURP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
